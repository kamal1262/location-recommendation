{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "from itertools import groupby\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm, tqdm_notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from distributed import Client\n",
    "# n_jobs = 8\n",
    "# client = Client(n_workers=n_jobs)\n",
    "# import modin.pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq_table = pq.read_table('data/web_seq.parquet')\n",
    "df = pq_table.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"data/sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clientId\"] = df[\"clientId\"].astype(str)\n",
    "df[\"visitMYDateTime\"] =  pd.to_datetime(df[\"visitMYDateTime\"], format='%Y-%m-%dT%H:%M:%S.%f')\n",
    "df[\"visitId\"] = df[\"visitId\"].astype(str)\n",
    "df[\"visitNumber\"] = df[\"visitNumber\"].astype(int)\n",
    "df[\"hitNumber\"] = df[\"hitNumber\"].astype(int)\n",
    "df[\"level_1\"] = df[\"level_1\"].astype(str)\n",
    "df[\"level_2\"] = df[\"level_2\"].astype(str)\n",
    "df[\"level_3\"] = df[\"level_3\"].astype(str)\n",
    "df[\"map_location_3\"] = df[\"map_location_3\"].astype(str)\n",
    "df[\"map_location_2\"] = df[\"map_location_2\"].astype(str)\n",
    "df[\"map_location_1\"] = df[\"map_location_1\"].astype(str)\n",
    "df[\"global_id\"] = df[\"global_id\"].astype(str)\n",
    "df[\"legacy_id\"] = df[\"legacy_id\"].astype(str)\n",
    "df[\"location_type\"] = df[\"location_type\"].astype(str)\n",
    "df[\"display_name\"] = df[\"display_name\"].astype(str)\n",
    "\n",
    "list_clientId = df.clientId.unique().tolist()\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_name_mapping(x):\n",
    "    if x[\"location_type\"] == \"BUILDING_NAME\":\n",
    "        return x[\"bld_display_name\"]\n",
    "    if x[\"location_type\"] == \"STREET_NAME\":\n",
    "        return x[\"str_display_name\"]\n",
    "    if x[\"location_type\"] == \"COUNTRY\":\n",
    "        return x[\"country_display_name\"]\n",
    "    if x[\"location_type\"] == \"REGION\":\n",
    "        return x[\"region_display_name\"]\n",
    "    if x[\"location_type\"] == \"STATE\":\n",
    "        return x[\"state_display_name\"]\n",
    "    if x[\"location_type\"] == \"DISTRICT\":\n",
    "        return x[\"district_display_name\"]\n",
    "    if x[\"location_type\"] == \"DIVISION\":\n",
    "        return x[\"div_display_name\"]\n",
    "    if x[\"location_type\"] == \"CITY\":\n",
    "        return x[\"city_display_name\"]\n",
    "    if x[\"location_type\"] == \"POST_CODE\":\n",
    "        return x[\"postcode_display_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location data\n",
    "location_table = pq.read_table(\"data/my_locations_db.parquet\")\n",
    "location_db_df = location_table.to_pandas()\n",
    "location_db_df[\"display_name\"] = location_db_df.apply(display_name_mapping, axis=1)\n",
    "location_db_df[\"map_location_1\"] = location_db_df[\"state_display_name\"]\n",
    "location_db_df[\"map_location_2\"] = location_db_df[\"city_display_name\"]\n",
    "location_db_df[\"map_location_3\"] = location_db_df[\"bld_display_name\"]\n",
    "location_db_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmploc_df = location_db_df.copy(deep=True)[[\"global_id\", \"legacy_id\", \"location_type\", \"display_name\", \"map_location_1\", \"map_location_2\", \"map_location_3\"]]\n",
    "tmploc_df.drop_duplicates(subset=[\"global_id\"], keep=\"first\", inplace=True)\n",
    "tmploc_df.set_index(\"global_id\", drop=True, inplace=True)\n",
    "dict_loc = tmploc_df.to_dict(orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('models/loc2name.p', 'wb') as f:\n",
    "#     pickle.dump(dict_loc, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of unique clientId: {df.clientId.nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = df.copy(deep=True)\n",
    "tmp_df[\"seq\"] = tmp_df[[\"clientId\", \"visitMYDateTime\", \"visitId\", \"visitNumber\", \"hitNumber\", \"global_id\"]]\\\n",
    "    .sort_values([\"visitMYDateTime\", \"visitId\", \"visitNumber\", \"hitNumber\"],ascending=True)\\\n",
    "    .groupby(\"clientId\")[\"global_id\"]\\\n",
    "    .transform(lambda x: '||'.join(x))\n",
    "tmp_df = tmp_df[[\"seq\"]].drop_duplicates(keep=\"first\")\n",
    "tmp_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_seq = []\n",
    "for v in tqdm(tmp_df.seq.values, total=len(tmp_df.seq.values), position=0, leave=True):\n",
    "    list_seq.append([x[0] for x in groupby(v.split(\"||\"))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc2idx = {w: idx for (idx, w) in enumerate(dict_loc)}\n",
    "idx2loc = {idx: w for (idx, w) in enumerate(dict_loc)}\n",
    "vocab_size = len(dict_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('models/loc2idx.p', 'wb') as f:\n",
    "#     pickle.dump(loc2idx, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('models/idx2loc.p', 'wb') as fp:\n",
    "#     pickle.dump(idx2loc, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 5\n",
    "idx_pairs = []\n",
    "\n",
    "for sequence in tqdm(list_seq, total=len(list_seq), position=0, leave=True):\n",
    "    indices = [loc2idx[location] for location in sequence]\n",
    "    \n",
    "    # For each word, threated as center word\n",
    "    for center_loc_pos in range(len(indices)):\n",
    "        # For each window position\n",
    "        for w in range(-window_size, window_size + 1):\n",
    "            context_loc_pos = center_loc_pos + w\n",
    "            # Make soure not jump out sentence\n",
    "            if context_loc_pos < 0 or context_loc_pos >= len(indices) or center_loc_pos == context_loc_pos:\n",
    "                continue\n",
    "            context_loc_idx = indices[context_loc_pos]\n",
    "            idx_pairs.append((indices[center_loc_pos], context_loc_idx))\n",
    "\n",
    "idx_pairs = np.array(idx_pairs)\n",
    "pairs_df = pd.DataFrame(idx_pairs, columns=[\"loc_1\", \"loc_2\"]).drop_duplicates(keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.functional as F\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(0)\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.randn(1,2)\n",
    "t2 = torch.randn(1,2).to(device)\n",
    "print(t1)\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('runs/skipgram2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_layer(word_idx):\n",
    "    x = torch.zeros(vocab_size).float()\n",
    "    x[word_idx] = 1.0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SkipGram #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dims = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = Variable(torch.randn(embedding_dims, vocab_size).float(), requires_grad=True)\n",
    "W2 = Variable(torch.randn(vocab_size, embedding_dims).float(), requires_grad=True)\n",
    "num_epochs = 51\n",
    "learning_rate = 0.001\n",
    "\n",
    "for epo in range(num_epochs):\n",
    "    loss_val = 0\n",
    "    for data, target in idx_pairs:\n",
    "        x = Variable(get_input_layer(data)).float()\n",
    "        y_true = Variable(torch.from_numpy(np.array([target])).long())\n",
    "\n",
    "        z1 = torch.matmul(W1, x)\n",
    "        z2 = torch.matmul(W2, z1)\n",
    "    \n",
    "        log_softmax = F.log_softmax(z2, dim=0)\n",
    "\n",
    "        loss = F.nll_loss(log_softmax.view(1,-1), y_true)\n",
    "        loss_val += loss.item()\n",
    "        loss.backward()\n",
    "        W1.data -= learning_rate * W1.grad.data\n",
    "        W2.data -= learning_rate * W2.grad.data\n",
    "\n",
    "        W1.grad.data.zero_()\n",
    "        W2.grad.data.zero_()\n",
    "    if epo % 10 == 0:    \n",
    "        print(f'Loss at epoch {epo}: {loss_val/len(idx_pairs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SkipGram #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"src/images/SkipGram-Negative-Sampling.png\" alt=\"drawing\" width=\"1080px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "import datetime\n",
    "import itertools\n",
    "\n",
    "from typing import Any\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "from src.config import MODEL_PATH\n",
    "from src.utils.logger import logger\n",
    "from src.ml.skipgram import SkipGram\n",
    "from src.utils.io_utils import save_model\n",
    "from src.ml.data_loader import Sequences, SequencesDataset\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"models/list_seq.p\", 'rb') as f:\n",
    "    list_seq = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle = True\n",
    "embedding_dims = 128\n",
    "epochs = 25\n",
    "initial_lr = 0.025\n",
    "batch_size = 16\n",
    "n_workers = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataloader\n",
    "sequences = Sequences(seq_list=list_seq, vocab_dict=dict_loc)\n",
    "dataset = SequencesDataset(sequences)\n",
    "dataloader = DataLoader(dataset, \n",
    "                        batch_size=batch_size, \n",
    "                        shuffle=shuffle, \n",
    "                        # num_workers=n_workers, \n",
    "                        collate_fn=dataset.collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "skipgram = SkipGram(vocab_size, embedding_dims).to(device)\n",
    "\n",
    "# Train loop\n",
    "optimizer = optim.SparseAdam(list(skipgram.parameters()), lr=initial_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "start_time = datetime.datetime.now()\n",
    "for epoch in tqdm(range(epochs), total=epochs, position=0, leave=True):\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, len(dataloader))\n",
    "    running_loss = 0\n",
    "    \n",
    "    # Training loop\n",
    "    for i, batches in enumerate(dataloader):\n",
    "        centers = batches[0].to(device)\n",
    "        contexts = batches[1].to(device)\n",
    "        neg_contexts = batches[2].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = skipgram.forward(centers, contexts, neg_contexts)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "        running_loss = running_loss * 0.9 + loss.item() * 0.1\n",
    "        \n",
    "    logger.info(\"Epoch: {}, Loss: {:.4f}, Lr: {:.6f}\".format(epoch, running_loss, optimizer.param_groups[0]['lr']))\n",
    "    results.append([epoch, i, running_loss])\n",
    "    running_loss = 0\n",
    "\n",
    "    # save model\n",
    "    current_datetime = datetime.datetime.now().strftime('%Y-%m-%d-%H%M')\n",
    "    state_dict_path = '{}/skipgram_epoch_{}_{}.pt'.format(MODEL_PATH, epoch, current_datetime)\n",
    "    torch.save(skipgram.state_dict(), state_dict_path)\n",
    "    logger.info('Model state dict saved to {}'.format(state_dict_path))\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "time_diff = round((end_time - start_time).total_seconds() / 60, 2)\n",
    "logger.info('Total time taken: {:,} minutes'.format(time_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results_df = pd.DataFrame(results, columns=['epoch', 'batches', 'loss'])\n",
    "results_df.to_csv('{}/model_metrics_w2v.csv'.format(MODEL_PATH), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize validation set\n",
    "val_samp = pd.read_csv('data/valid_model.csv')\n",
    "\n",
    "# Get product ID\n",
    "val_samp['loc1_id'] = val_samp['loc_1'].values\n",
    "val_samp['loc2_id'] = val_samp['loc_2'].values\n",
    "val_samp = val_samp[(val_samp['loc1_id'] > -1) & (val_samp['loc2_id'] > -1)]  # Keep those with valid ID\n",
    "logger.info('No. of validation samples: {}'.format(val_samp.shape[0]))\n",
    "\n",
    "loc1_id = val_samp['loc1_id'].values\n",
    "loc2_id = val_samp['loc2_id'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.functional as F\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "import datetime\n",
    "import itertools\n",
    "\n",
    "from typing import Any\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "from src.config import MODEL_PATH\n",
    "from src.utils.logger import logger\n",
    "from src.ml.skipgram import SkipGram\n",
    "from src.utils.io_utils import save_model\n",
    "from src.ml.data_loader import Sequences, SequencesDataset\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SkipGram(\n",
       "  (center_embeddings): Embedding(14699, 128, sparse=True)\n",
       "  (context_embeddings): Embedding(14699, 128, sparse=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 14699 # 6273\n",
    "embedding_dims = 128\n",
    "\n",
    "skipgram = SkipGram(vocab_size, embedding_dims).to(device)\n",
    "skipgram.load_state_dict(torch.load(\"models/skipgram_epoch_24_2020-11-16-1439.pt\", map_location=device))\n",
    "skipgram.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skipgram.save_embeddings(\"models/embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_vec = np.load(\"models/embeddings.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"models/idx2loc.p\", 'rb') as f:\n",
    "    dict_idx2loc = pickle.load(f)\n",
    "with open(\"models/loc2idx.p\", 'rb') as fp:\n",
    "    dict_loc2idx = pickle.load(fp)\n",
    "with open(\"models/loc2name.p\", 'rb') as p:\n",
    "    dict_loc2name = pickle.load(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_idx2name = {}\n",
    "for x in dict_idx2loc.keys():\n",
    "    dict_idx2name[x] = dict_loc2name[dict_idx2loc[x]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>loc_id</th>\n",
       "      <th>loc_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bld_67258</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>str_66918</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>str_66979</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bld_66962</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bld_66917</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      loc_id loc_idx\n",
       "0  bld_67258       0\n",
       "1  str_66918       1\n",
       "2  str_66979       2\n",
       "3  bld_66962       3\n",
       "4  bld_66917       4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "metadata_df = pd.DataFrame.from_dict(dict_idx2loc, orient='index')\n",
    "metadata_df['loc_idx'] = metadata_df.index\n",
    "metadata_df.columns = [['loc_id', 'loc_idx']]\n",
    "# metadata_df = metadata_df[[\"loc_id\"]].replace(to_replace='None', value=np.nan).dropna()\n",
    "# metadata_df.set_index(metadata_df[['loc_id']].values.reshape(-1,).tolist(), drop=False, verify_integrity=True)\n",
    "metadata_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loc_id</th>\n",
       "      <th>full_address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bld_67258</th>\n",
       "      <td>bld_67258</td>\n",
       "      <td>The Amber Residence, Kota Kemuning, Selangor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>str_66918</th>\n",
       "      <td>str_66918</td>\n",
       "      <td>None, KL City, Kuala Lumpur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>str_66979</th>\n",
       "      <td>str_66979</td>\n",
       "      <td>None, Batu Caves, Kuala Lumpur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bld_66962</th>\n",
       "      <td>bld_66962</td>\n",
       "      <td>Desa Bakti, Batu Caves, Kuala Lumpur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bld_66917</th>\n",
       "      <td>bld_66917</td>\n",
       "      <td>Flat DBKL Jalan Hang Tuah, KL City, Kuala Lumpur</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              loc_id                                      full_address\n",
       "bld_67258  bld_67258      The Amber Residence, Kota Kemuning, Selangor\n",
       "str_66918  str_66918                       None, KL City, Kuala Lumpur\n",
       "str_66979  str_66979                    None, Batu Caves, Kuala Lumpur\n",
       "bld_66962  bld_66962              Desa Bakti, Batu Caves, Kuala Lumpur\n",
       "bld_66917  bld_66917  Flat DBKL Jalan Hang Tuah, KL City, Kuala Lumpur"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_meta_df = pd.DataFrame.from_dict(dict_loc2name, orient='index')\n",
    "name_meta_df['loc_id'] = name_meta_df.index\n",
    "name_meta_df['full_address'] = name_meta_df.apply(lambda x: f\"{x['map_location_3'] }, { x['map_location_2'] }, { x['map_location_1'] }\", axis=1)\n",
    "name_meta_df[[\"loc_id\", \"full_address\"]].head()\n",
    "\n",
    "# .to_csv('models/metadata.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_node(loc_type, index, nodes, top_n=5, filter_type=False):\n",
    "    node = nodes[index]\n",
    "    closest_index = distance.cdist([node], nodes, \"cosine\")[0]\n",
    "    result = zip(range(len(closest_index)), closest_index)\n",
    "    result = sorted(result, key=lambda x: x[1])\n",
    "    \n",
    "    location_src = dict_loc2name[dict_idx2loc[index]]\n",
    "    print(f\"Finding location near to: { dict_idx2loc[index] } ({location_src['map_location_3']}, {location_src['map_location_2']}, {location_src['map_location_1']}) - is a {location_src['location_type']} \\n\")\n",
    "    \n",
    "    if not filter_type:\n",
    "        cnt = 1\n",
    "        for idx, dist in result[1:top_n+1]:\n",
    "            location = dict_loc2name[dict_idx2loc[idx]]\n",
    "            l1 = location[\"map_location_1\"]\n",
    "            l2 = location[\"map_location_2\"]\n",
    "            l3 = location[\"map_location_3\"]\n",
    "\n",
    "            print(f\"{cnt} ==> {dict_idx2loc[idx]} - {l3}, {l2}, {l1}, (score: {dist})\")\n",
    "            cnt += 1\n",
    "    \n",
    "    if filter_type:\n",
    "        cnt_loc_type = 0\n",
    "        idx_loc_type = 1\n",
    "        while cnt_loc_type < top_n+1:\n",
    "            \n",
    "            idx, dist = result[idx_loc_type]\n",
    "            location = dict_loc2name[dict_idx2loc[idx]]\n",
    "            l1 = location[\"map_location_1\"]\n",
    "            l2 = location[\"map_location_2\"]\n",
    "            l3 = location[\"map_location_3\"]\n",
    "\n",
    "            if location[\"location_type\"] == loc_type:\n",
    "                print(f\"{cnt_loc_type} ==> {dict_idx2loc[idx]} - {l3}, {l2}, {l1}, (score: {dist})\")\n",
    "                cnt_loc_type += 1\n",
    "            idx_loc_type += 1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding location near to: ctr_23684 (None, None, None) - is a COUNTRY \n",
      "\n",
      "1 ==> bld_62664 - Taman Tasek Emas, Kampar, Perak, (score: 0.6764543054944703)\n",
      "2 ==> str_57702 - None, Kepong, Kuala Lumpur, (score: 0.6936324195169968)\n",
      "3 ==> mycty_51971 - None, Enggor, Perak, (score: 0.6957895923870276)\n",
      "4 ==> str_57170 - None, Cheras, Selangor, (score: 0.6981588830398032)\n",
      "5 ==> bld_63393 - Desa Damansara 2, Damansara Heights, Kuala Lumpur, (score: 0.7030721580953188)\n",
      "6 ==> ptc_53223 - None, None, Selangor, (score: 0.7048956372812158)\n",
      "7 ==> bld_62122 - Mont Kiara Damai Resort Condominium, Mont Kiara, Kuala Lumpur, (score: 0.7106526514577839)\n",
      "8 ==> str_59687 - None, Jelutong, Penang, (score: 0.7129465472031167)\n",
      "9 ==> ptc_52514 - None, None, Penang, (score: 0.7135933383524986)\n",
      "10 ==> str_60183 - None, Salak Selatan, Kuala Lumpur, (score: 0.7171585430411767)\n"
     ]
    }
   ],
   "source": [
    "closest_node(\"COUNTRY\", 5, emb_vec, top_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_name2idx = {f\"{ v['display_name'] } - { v['location_type'] }\": k for k, v in dict_idx2name.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BUILDING_NAME',\n",
       " 'STREET_NAME',\n",
       " 'COUNTRY',\n",
       " 'REGION',\n",
       " 'STATE',\n",
       " 'DISTRICT',\n",
       " 'DIVISION',\n",
       " 'CITY',\n",
       " 'POST_CODE']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "loc_type_list = pd.DataFrame(dict_idx2name).transpose()[\"location_type\"].unique().tolist()\n",
    "loc_type_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bld_list = []\n",
    "for building in list(dict_name2idx.keys()):\n",
    "    if building.endswith(\"BUILDING_NAME\"):\n",
    "        bld_list.append(building)\n",
    "\n",
    "ct_list = []\n",
    "for city in list(dict_name2idx.keys()):\n",
    "    if city.endswith(\"CITY\"):\n",
    "        ct_list.append(city)\n",
    "\n",
    "state_list = []\n",
    "for state in list(dict_name2idx.keys()):\n",
    "    if city.endswith(\"CITY\"):\n",
    "        state_list.append(city)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b432dd69009442bebce304ff1755a83a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='x', options=('Air Tawar - CITY', 'Alai - CITY', 'Alam Impian - CIT…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(x=sorted(ct_list), loc_type=loc_type_list)\n",
    "def dropdown_idx(x, loc_type):\n",
    "    idx = dict_name2idx[x]\n",
    "    closest_node(loc_type, idx, emb_vec, top_n=20, filter_type=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8af39cdd3d74aeb8b3352fe7e8ec36d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='x', options=('Air Tawar - CITY', 'Alai - CITY', 'Alam Impian - CIT…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(x=sorted(ct_list), y=sorted(ct_list))\n",
    "def minus_vec(x,y):\n",
    "    idx_1 = dict_name2idx[x]\n",
    "    idx_2 = dict_name2idx[y]\n",
    "\n",
    "    emb_idx_1 = emb_vec[idx_1]\n",
    "    emb_idx_2 = emb_vec[idx_2]\n",
    "    diff_emb = emb_idx_1 - emb_idx_2\n",
    "    \n",
    "#     print(emb_idx_1)\n",
    "\n",
    "    closest_index = distance.cdist([diff_emb], emb_vec, \"cosine\")[0]\n",
    "    result = zip(range(len(closest_index)), closest_index)\n",
    "    result = sorted(result, key=lambda x: x[1])\n",
    "\n",
    "    print(f\"We try ({x}) - ({y}) ...\")\n",
    "    print(f\"Finding location near to: ({x}) - ({y}) \\n\")\n",
    "\n",
    "    cnt = 0\n",
    "    for idx, dist in result[1:6]:\n",
    "        location = dict_loc2name[dict_idx2loc[idx]]\n",
    "        l1 = location[\"map_location_1\"]\n",
    "        l2 = location[\"map_location_2\"]\n",
    "        l3 = location[\"map_location_3\"]\n",
    "\n",
    "        print(f\"{cnt} ==> {dict_idx2loc[idx]} - {l3}, {l2}, {l1}, (score: {dist})\")\n",
    "        cnt =+ 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(comment=\"EMB.SKIPGRAM.v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14699/14699 [00:00<00:00, 992100.07it/s]\n"
     ]
    }
   ],
   "source": [
    "meta_list = []\n",
    "for index in tqdm(dict_idx2name.keys()):\n",
    "    \n",
    "    location_type = dict_idx2name[index]['location_type']\n",
    "    map_location_1 = dict_idx2name[index]['map_location_1']\n",
    "    map_location_2 = dict_idx2name[index]['map_location_2']\n",
    "    map_location_3 = dict_idx2name[index]['map_location_3']\n",
    "\n",
    "    meta = f\"{location_type} - {map_location_3}, {map_location_2}, {map_location_1}\"\n",
    "    meta_list.append(meta)\n",
    "    \n",
    "writer.add_embedding(torch.tensor(emb_vec), metadata=meta_list, tag=\"locations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 8670), started 0:15:52 ago. (Use '!kill 8670' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-8643fe40b646bbe4\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-8643fe40b646bbe4\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir ./runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
